\documentclass[letterpaper,10pt]{article}
\usepackage{fullpage}
\usepackage{textcomp}
\usepackage{times}
\usepackage{cite}
\usepackage{fancyvrb}
\usepackage{moreverb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multicol}

\newcommand{\squishlist}{\begin{list}{$\bullet$}
  {\setlength{\itemsep}{0pt}
    \setlength{\parsep}{3pt}
    \setlength{\topsep}{3pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\labelwidth}{1em}
    \setlength{\labelsep}{0.5em}}}

\newcommand{\squishend}{\end{list}}

\title{System-Centric Threat Modeling of CUDA\footnote{CUDA\textsuperscript{\textregistered}
is a registered trademark of the NVIDIA Corporation.}}
\author{Nick Black and Jason Rodzik\\
CS8803SS Project, Spring 2010}
\date{}

\begin{document}
\maketitle

\begin{abstract}
The programmable, massively parallel graphics cards of modern laptops, desktops
and high-performance workstations are sources of tremendous computing power.
The hundreds of simple in-order cores found on recent NVIDIA and ATI graphics
hardware provide many more peak FLOPS than the x86 processor packages with
which they're commonly coupled.
Devices such as NVIDIA's Tesla\texttrademark{} have already traded video outputs for onboard memory, giving rise to ``desktop
supercomputing.'' Those who would fully utilize their machines can
no more ignore heterogeneous processors than multiple cores or SIMD. Leaving
large matrix multiplications on general-purpose, out-of-order CPUs is about as
efficient as eschewing hardware multipliers in favor of iterated addition.

It is, however, almost certainly safer. GPUs have traditionally accelerated
single-instance, interactive tasks (such as video games and CAD) and
decomposable, compute-bound tasks (such as batch rendering) for which a
deinterleaved schedule is optimal. Hardware facilities for the support of
multiple threads or processes are primitive at best. When the GPU was limited to
blitting, fogging and aliasing framebuffers, minimal isolation and
assurance capabilities were tolerable. Compromises of scientific data,
cryptographic materials and SCADA systems are rather more serious, yet these
precise applications are driving the GPGPU's adoption. We provide a system-centric threat
model analysis of NVIDIA's CUDA system. We attack the system, drawing blood.
We close with software-level defenses against our attacks, and their limitations.
Our results suggest that, unless GPGPU systems add complexity related to security state (and thus devote
less transistor budget to performance), they cannot provide a trusted computing
base.

\end{abstract}

\begin{multicols}{2}
\section{Introduction}
Heterogeneous computing has definitely arrived, and graphics processing units (GPUs) in the millions are employed worldwide. Tradition has shown newly programmable domains to be rapidly subjected (and often found vulnerable) to attacks of the past; indeed, wherever processing goes, so does the possibility of automated attacks. With high powered GPU's moving from the gamer's desktop to the laboratory's cluster, it's essential that the security issues surrounding their use be fleshed out earlier rather than later. Unfortunately, this is not the case; graphics card manufacturers have not publicized any of their own internal security studies (if any exist), and popular manufacturers are infamous for their
hostility to open source and academia\footnote{See for instance Wikipedia's page on
``\href{http://en.wikipedia.org/wiki/NVIDIA\_and\_FOSS}{Graphics hardware and FOSS}''.}
% This needs to be revised a bit and needs to have more added to it...
We have identified a variety of attack paths on existing GPU platforms from NVIDIA and ATI in addition to methods for countering these attacks.

\section{Threat Model}
Well-known threat taxonomies include the ``CIA Triad'' (extended by the
``Parkerian Hexad\cite{sechandbook}'') and Microsoft's STRIDE. The latter, developed as part of
Microsoft's Secure Product Lifecycle, is designed for system-centric threat
modeling and suitable for our purposes.
\subsection{Spoofing}
\subsection{Tampering}
\subsection{Repudiation}
\subsection{Information disclosure}
\subsection{Denial of service}
\subsection{Escalation of privilege}

\section{Attacking CUDA}

\section{Defending CUDA}

\section{Conclusions} 
The recommendations of the Orange Book\cite{orangebook}, requirements of the Common Criteria,
and Saltzer-Schroeder Practices have long provided general principles for
security-conscious design. Any number of these principles have been ignored
(or poorly effected) along the way to NVIDIA's and ATI's current GPUs. These
manufacturers focused nigh-exclusively on performance relative to cost and
power, resulting in some of the world's most powerful devices. Harnessing these
mighty processing units is necessary to even approach full machine utilization.
This will become only more true:
\begin{itemize}
\item Increasing the clock frequency exacerbates cooling and power requirements
(dynamic power consumption is proportional to frequency). Furthermore,
clock frequency increases tend to force decomposition of pipeline
stages, exacerbating branch misprediction delays and pressuring any OOO
system\cite{cormean}.
\item Increasing the issue width, or duplicating functional units, either
requires ISA changes (for VLIW) or massive frontend resources for lookahead,
disambiguation, and hazard tracking. Assuming these expenses acceptable,
diminishing returns result from limitations of instruction-level parallelism.
Furthermore, very wide issues lead to sparse code flows, taxing the instruction
store subsystem. 
\item Further investments in cache --- and thus, hopefully, fewer memory
delays --- serves only to approach more closely a device's theoretical peak.
That peak itself is a function of architecture, not of programs or their
access patterns.
\item Investments in OOO apparatus (reorder buffers,
frontend reservation stations, register renaming) yield diminishing returns due
to the profound limitations of instruction level parallelism\cite{phenn}. Like improvements
to cache, OOO can only hide delays, not find new FLOPS.
\item Denser chip-multithreading similarly exists only to hide latency.
\item Larger chips require either pipelined wires or high voltages to operate
reliably, and place strict requirements on clock-signaling circuity. Efficient
power management requires complex and expensive partial power gating\footnote{For
instance, feeding $\mu$ops from Nehalem's Loop Stream Detector results in power-down
of the leading three pipeline stages.}
\end{itemize}
We see that FLOPS --- at any price --- can be had only by adding cores or extending
SIMD order. The former is obviously easier with small, in-order, SISD cores,
such as the unified shaders of a modern GPU; duplicating a Nehalem core is a
much more daunting process. As for the latter, note that Intel's ``Sandy
Bridge'' microarchitecture is expected to add the AVX instruction scheme and
its 256-bit \texttt{YMM} registers. The data-based decomposition of CUDA's
``SIMT'' model can immediately take advantage of these new cores (given
sufficiently large problem sets, of course), whereas x86 binaries\footnote{Most of them, anyway.} would
require dynamic translation or recompilation to take advantage of the new SIMD
order.

Extensive manycoring and huge memory bandwidths are certainly the key to FLOPS,
but the current state of programmable GPUs are damningly insecure. Without the
addition of infrastructure and policy, they must not be trusted in a
multiprocessing context. Even in the absence of calculated attack, one's own
mistakes or accidents can wreck undetected havoc. It ought be noted that memory
protection could be implemented without the added complexity of
hardware-supported virtual memory (provided some assistance from the OS); this
would immediately address most issues.

\section{Related Work}
Our research in this area is focused on the possibility of security
vulnerabilities agaiynst the GPU itself, aan area which we were unable to find
prior research for. However, our research is motivated in part by an increase
in applications being developed for GPUs and also by GPUs being used as the
processing unit for security related tasks.
  
\subsection{General applications}
  
  Dedicated graphical processing units (GPUs) have seen significant advancement
in the past couple decades due to the insatiable demand that consumers have for
ever-increasing advances in video game graphics. This development has led to
recent GPUs reaching the point where they are powerful enough that there has
been significant research invested into using them to execute tasks outside the
realm of video games. One of the most popular examples of this is the
Folding@Home project, which in recent years has developed a version of its
application that runs on both ATI and nVidia brand GPUs
\url{http://folding.stanford.edu/English/Guide#ntoc4}.
  
  In 2007, nVidia released the SDK for CUDA, their "Compute Unified Device
Architecture," allowing for the GPU to be much more accessible to developers
wishing to use the GPU for non-graphical applications. In the three years since
then, a wide variety of applications for CUDA have been developed\footnote{\url{http://www.nvidia.com/object/what\_is\_cuda\_new.html}}.
  
  General Mills developed an application that used CUDA to model the optimal
way to cook a frozen pizza in a microwave. SeismicCity used CUDA to improve the
amount of time it takes to interpret seismic data to determine the optimal
drilling locations for finding oil
\url{http://www.nvidia.com/object/cuda\_in\_action.html}.
  
  Other companies are offering CUDA solutions for problems in the realms of
electromagnetics, bioinformatics, finance, accelerator physics, aerodynamics,
engine optimizations, image and video stream compression, healthcare and life
sciences, medical imaging, defense, and more
\squishlist
\item \url{http://www.acceleware.com/default/index.cfm/professional-services/}
\item \url{http://www.aneo.fr/index.php?option=com\_content&amp;view=article&amp;id=91&amp;Itemid=110}
\item \url{http://www.parallel-compute.com/}
\item \url{http://www.caps-entreprise.com/nvidia.html}
\item \url{http://www.elegant-mathematics.com/index.html?NVGPU}
\item \url{http://www.culatools.com/consulting}
\item \url{http://www.fixstars.com/en/solutions/gpu/}
\item \url{http://www.hoopoe-cloud.com/Services.aspx}
\item \url{http://www.hpc-project.com/gpu2.htm}
\item \url{http://www.sagivtech.com/21541.html}
\item \url{http://www.stoneridgetechnology.com/services/visualcomputing.asp}
\item \url{http://gpucomputing.txcorp.com/}.
\squishend
  
\subsection{Security applications}
  There are a few research areas where GPUs have been used in security applications, but all of these involve using the GPU as a faster processor than the CPU, and none of the research involves investigating the GPU itself or coding frameworks such as CUDA from a security standpoint.
  
  The most common security task that GPUs have been used for in prior work have been to use the GPU for performing cryptographic computations. Research has found that GPUs can perform some AES-related OpenSSL computations up to 20 times as fast as a typical implementation
\url{http://www.manavski.com/downloads/PID505889.pdf}. Another aspect that is of importance, particularly to copyright groups such as the RIAA and MPAA, are the encryption techniques used with GPUs for the security of applications involving remote displays, such as for HDCP blu-ray implementations\url{http://www.amazon.com/CryptoGraphics-Exploiting-Graphics-Security-Information/dp/038729015X}.
  
  While GPUs can be used for improving the speed of cipher implementations, they can be used to speed up cryptographic attacks as well. In some cases, applications have been developed using CUDA to allow for WiFi keys to be broken up to 100 times as fast as in typical implementations \url{http://arstechnica.com/security/news/2008/10/company-puts-nvida-gpus-to-work-cracking-wireless-security.ars}.
  
  Intrusion detection systems can exhibit performance degradation under heavy
  loads, and some of the proposed solutions to this problem have involved
  offloading IDS computations to the GPU
  \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.3302&amp;rep=rep1&amp;type=pdf}.
  Additionally, some techniques in pattern-matching are difficult to perform
  quickly enough to be useful, and GPUs have been proposed as a solution that
  allow for significant decreases in computation time
  \url{http://portal.acm.org/citation.cfm?id=1395492}.

\bibliographystyle{acm}
\bibliography{cs8803ssProject}
\end{multicols}
\end{document}
